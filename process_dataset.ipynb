{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stackoverflow Questions tagging with multiple labels.<br>\n",
    "Dataset - Hackerearth Deeplearning Challenege #4 Dataset.<br>\n",
    "Architecture - Nvidia GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops \n",
    "\n",
    "from collections import Counter \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from _operator import index\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    global EMPTY\n",
    "    EMPTY = ''\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    text = re.sub('<pre><code>.*?</code></pre>',EMPTY,text)\n",
    "    text = re.sub('<a[^>]+>(.*)</a>', replace_link, text)\n",
    "    return re.sub('<[^>]+>', EMPTY, text)\n",
    "\n",
    "def replace_link(match):\n",
    "    return EMPTY if re.match('[a-z]+://',match.group(1)) else match.group(1)\n",
    "\n",
    "def process_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    DATA = pd.DataFrame(columns=['article','tags'])\n",
    "    DATA['article'] = df['article'].apply(clean_text).str.lower() \n",
    "    DATA['article'] = DATA['article'].apply(lambda x: x.replace('\"','').replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\"\\r\",\"\"))\n",
    "    DATA['tags'] = df['tags'].values\n",
    "    DATA = DATA.dropna()\n",
    "    \n",
    "    return DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to read dataset84.92910122871399\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "filename = './new_dataset/train.csv'\n",
    "train_data = process_data(filename)\n",
    "end = time.time()\n",
    "\n",
    "elapsed = end - start \n",
    "print(\"Time to read dataset\"+str(elapsed))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_data['article']\n",
    "train_Y = train_data['tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1> Preprocess features dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, RepeatVector, TimeDistributed\n",
    "\n",
    "## Plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "# Others\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import *\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    text = text.lower().split()\n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    text = [str(TextBlob(w)) for w in text]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    text = re.sub(r'(\\s\\d+\\s)','',text)\n",
    "    text = re.sub(r'[0-9]*','',text)\n",
    "    text = re.sub(r'\\+','',text)\n",
    "    text = re.sub(r'\\=','',text)\n",
    "    text = re.sub(r'\\;','',text)\n",
    "    text = re.sub(r'\\-','',text)\n",
    "    text = re.sub(r'\\:','',text)\n",
    "    text = re.sub(r'\\!','',text)\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "\n",
    "    \n",
    "    ## Stemming\n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    \n",
    "    ##Lemmatization\n",
    "#     text = text.split()\n",
    "#     lem_words = [Word(word).lemmatize() for word in text]\n",
    "#     text = \" \".join(lem_words)\n",
    "\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'code produc thisselect end date exhibit room placeofexhibit end date gtcurrent date minimum andr maximum locat name name exhibit orderbi end date desc column row requir second biggest one alway need second biggest one though depend count room room queri return need fourth biggest one requir get subquerythi subqueri haveselect end date exhibit room placeofexhibit end date gtcurrent date minimum andr maximum locat name name exhibit orderbi end date asc offset select count name room minimum maximum limit return queri get second last date wrong direct get second smallest instead second largest'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_text(train_X[1])\n",
    "# train_data['article'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when required to process text \n",
    "train_X = train_X.map(lambda x: process_text(x))\n",
    "Saving_train = pd.DataFrame(columns=['article'])\n",
    "Saving_train['article'] = train_X\n",
    "path = r'C:\\Users\\adilf\\Deeplearning Projects\\hackerearth#4\\process-dataset'\n",
    "Saving_train.to_csv(os.path.join(path,r'train_X.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Process tags and save them to csv </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process labels\n",
    "def process_labels(text):\n",
    "    text = text.replace(\"|\",\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when required to process labels\n",
    "train_Y = train_Y.map(lambda x: process_labels(x))\n",
    "Saving_temp = pd.DataFrame(columns=['tags'])\n",
    "Saving_temp['tags'] = train_Y\n",
    "path = r'C:\\Users\\adilf\\Deeplearning Projects\\hackerearth#4\\process-dataset'\n",
    "Saving_temp.to_csv(os.path.join(path,r'train_Y.csv'),encoding='utf-8',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tags(labels):\n",
    "    tags = []\n",
    "    for label in labels.values:\n",
    "        tag = label.split(\" \")\n",
    "        tags.extend(tag)\n",
    "    tags_set = list(set(tags))\n",
    "    tags_df = pd.DataFrame(tags_set,columns=['tags'])\n",
    "    print(tags_df['tags'].value_counts())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_tags(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "755"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1302"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data['article'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create X sequence\n",
    "vocabulary_size = 30000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(train_X)\n",
    "sequences = tokenizer.texts_to_sequences(train_X)\n",
    "seq_x = pad_sequences(sequences,maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  243,   590,   556,   622,   276,   214,   322,  3361,   622,\n",
       "         276,   533,  3652,   355,    74,   130,   698,  1399,  2138,\n",
       "          58,   504,   244, 13202,   116,   721,  2138,   721,    58,\n",
       "         322,  3067,   282,   157,   304,  1714,   721,   128,    98,\n",
       "         104,   145,    68,    38, 10257,  1803,  4228,    59,   157,\n",
       "        3361,  3059, 26261,    37,  3474,  1143,   328,  3157,  1482,\n",
       "          17,  3288,   637,   328,  1143,  1849,  3157,  1482,    17,\n",
       "         588,   157,   693,  1586,    54,   637,    54,   637,  1311,\n",
       "         461,   304, 15510,   166,  1965,   453,  1080,  5177, 11552,\n",
       "         157,   253,   460,   622,   244,   328,   434,   205,    79,\n",
       "          78,  2168,  3808,   259,  1335,   511,    79,   102,   915,\n",
       "          79])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Y sequence\n",
    "vocabulary_size = 36330\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(train_Y)\n",
    "sequences = tokenizer.texts_to_sequences(train_Y)\n",
    "data_Y = pad_sequences(sequences, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
